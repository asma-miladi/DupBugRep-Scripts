{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled3.ipynb","provenance":[],"authorship_tag":"ABX9TyN0KlKHDQ+uzQoks65sNPll"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"rdgORB-Fk3nu","executionInfo":{"status":"error","timestamp":1608075263511,"user_tz":-60,"elapsed":912,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"12499fee-fbc6-4273-ecaa-509622e13b7f"},"source":["import numpy as np\r\n","from pickle import load\r\n","from numpy import array\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.utils.vis_utils import plot_model\r\n","from keras.models import Model\r\n","from keras.layers import Input\r\n","from keras.layers import Dense\r\n","from keras.layers import Flatten\r\n","from keras.layers import Dropout\r\n","from keras.layers import Embedding\r\n","from keras.layers.convolutional import Conv1D\r\n","from keras.layers.convolutional import MaxPooling1D\r\n","from keras.layers.merge import concatenate\r\n","\r\n","# load a clean dataset\r\n","def load_dataset(filename):\r\n","\treturn load(open(filename, 'rb'))\r\n","\r\n","# fit a tokenizer\r\n","def create_tokenizer(lines):\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n","\r\n","# calculate the maximum document length\r\n","def max_length(lines):\r\n","  for s in lines:\r\n","    s=str(s)\r\n","    Type = s.split(\",\")\r\n","  return max([len(Type) for s in lines])\r\n","\t#return max([len(Type) for s in lines])\r\n","\r\n","# encode a list of lines\r\n","def encode_text(tokenizer, lines, length):\r\n","\t# integer encode\r\n","\tencoded = tokenizer.texts_to_sequences(lines)\r\n","\t# pad encoded sequences\r\n","\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n","\treturn padded\r\n","\r\n","# define the model\r\n","def define_model(length, vocab_size):\r\n","\t# channel 1\r\n","\tinputs1 = Input(shape=(length,))\r\n","\tembedding1 = Embedding(vocab_size, 40)(inputs1)\r\n","\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n","\tdrop1 = Dropout(0.5)(conv1)\r\n","\tpool1 = MaxPooling1D(pool_size=2)(drop1)\r\n","\tflat1 = Flatten()(pool1)\r\n","\t# channel 2\r\n","\tinputs2 = Input(shape=(length,))\r\n","\tembedding2 = Embedding(vocab_size, 40)(inputs2)\r\n","\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\r\n","\tdrop2 = Dropout(0.5)(conv2)\r\n","\tpool2 = MaxPooling1D(pool_size=2)(drop2)\r\n","\tflat2 = Flatten()(pool2)\r\n","\t# channel 3\r\n","\tinputs3 = Input(shape=(length,))\r\n","\tembedding3 = Embedding(vocab_size,40)(inputs3)\r\n","\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\r\n","\tdrop3 = Dropout(0.5)(conv3)\r\n","\tpool3 = MaxPooling1D(pool_size=2)(drop3)\r\n","\tflat3 = Flatten()(pool3)\r\n","\t# merge\r\n","\tmerged = concatenate([flat1, flat2, flat3])\r\n","\t# interpretation\r\n","\tdense1 = Dense(10, activation='relu')(merged)\r\n","\toutputs = Dense(1, activation='sigmoid')(dense1)\r\n","\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\r\n","\t# compile\r\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\t# summarize\r\n","\tprint(model.summary())\r\n","\tplot_model(model, show_shapes=True, to_file='multichannel.png')\r\n","\treturn model\r\n","\r\n","# load training dataset\r\n","trainLines, trainLabels = load_dataset('train.pkl')\r\n","# create tokenizer\r\n","tokenizer = create_tokenizer(trainLines)\r\n","# calculate max document length\r\n","#length = 40\r\n","length = max_length(trainLines)\r\n","# calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print(length)\r\n","print('Max document length: %d' % length)\r\n","print('Vocabulary size: %d' % vocab_size)\r\n","# encode data\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","trainX=np.reshape(trainX,(5, 1108700))\r\n","trainLabels=np.reshape(trainLabels,(5, 8))\r\n","print(trainX.shape)\r\n","\r\n","# define model\r\n","model = define_model(length, vocab_size)\r\n","# fit model\r\n","model.fit([trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\r\n","# save the model\r\n","model.save('model.h5')\r\n"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d57b9a10ffe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# load training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrainLines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;31m# create tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-d57b9a10ffe5>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# load a clean dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# fit a tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.pkl'"]}]}]}