{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled2.ipynb","provenance":[],"authorship_tag":"ABX9TyMtj9m4ZUV32q0eUpYKMjgy"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"mg6KpUUPua9X"},"source":["from tensorflow.keras import optimizers\r\n","from tensorflow.python.keras.optimizers import TFOptimizer\r\n","import numpy as np\r\n","from nltk.corpus import stopwords\r\n","import string\r\n","from pickle import load\r\n","from numpy import array\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.utils.vis_utils import plot_model\r\n","from keras.models import Model\r\n","from keras.layers import Input\r\n","from keras.layers import Dense\r\n","from keras.layers import Flatten\r\n","from keras.layers import Dropout\r\n","from keras.layers import Embedding\r\n","from keras.layers.convolutional import Conv1D\r\n","from keras.layers.convolutional import Conv2D\r\n","from keras.layers.convolutional import MaxPooling2D\r\n","from keras.layers.convolutional import MaxPooling1D\r\n","from keras.layers.normalization import BatchNormalization\r\n","from keras.layers.merge import concatenate\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ym_IHP5YbauG"},"source":["# load doc into memory\r\n","def load_doc(filename):\r\n","\t# open the file as read only\r\n","\tfile = open(\"/Users/user/Desktop/latest/dccnn-master/test.txt\", 'r')\r\n","\t# read all text\r\n","\ttext = file.read()\r\n","\t# close the file\r\n","\tfile.close()\r\n","\treturn text\r\n","\r\n","# turn a doc into clean tokens\r\n","def clean_doc(doc):\r\n","\t# split into tokens by white space\r\n","\ttokens = doc.split()\r\n","\t# remove punctuation from each token\r\n","\ttable = str.maketrans('', '', string.punctuation)\r\n","\ttokens = [w.translate(table) for w in tokens]\r\n","\t# remove remaining tokens that are not alphabetic\r\n","\ttokens = [word for word in tokens if word.isalpha()]\r\n","\t# filter out stop words\r\n","\t#stop_words = set(stopwords.words('english'))\r\n","\t#tokens = [w for w in tokens if not w in stop_words]\r\n","\t# filter out short tokens\r\n","\ttokens = [word for word in tokens if len(word) > 1]\r\n","\treturn tokens\r\n","\r\n","# load the document\r\n","filename = 'data.txt'\r\n","text = load_doc(filename)\r\n","tokens = clean_doc(text)\r\n","#print(tokens)\r\n","\r\n","\r\n","from os import listdir\r\n","def process_docs(directory, is_trian):\r\n","\tdocuments = list()\r\n","\t# walk through all files in the folder\r\n","\tfor filename in listdir(directory):\r\n","\t\t# skip any reviews in the test set\r\n","\t\tif is_trian and filename.startswith('cv9'):\r\n","\t\t\tcontinue\r\n","\t\tif not is_trian and not filename.startswith('cv9'):\r\n","\t\t\tcontinue\r\n","\t\t# create the full path of the file to open\r\n","\t\tpath = directory + '/' + filename\r\n","\t\t# load the doc\r\n","\t\tdoc = load_doc(path)\r\n","\t\t# clean doc\r\n","\t\ttokens = clean_doc(doc)\r\n","\t\t# add to list\r\n","\t\tdocuments.append(tokens)\r\n","\treturn documents\r\n","from pickle import dump\r\n","def save_dataset(dataset, filename):\r\n","\tdump(dataset, open(filename, 'wb'))\r\n","\tprint('Saved: %s' % filename)\r\n","\r\n","# load all training reviews\r\n","negative_docs = process_docs('/Users/user/Desktop/txt_sentoken/neg', True)\r\n","positive_docs = process_docs('/Users/user/Desktop/txt_sentoken/pos', True)\r\n","trainX = negative_docs + positive_docs\r\n","trainy = [0 for _ in range(40)] + [1 for _ in range(40)]\r\n","save_dataset([trainX,trainy], 'train.pkl')\r\n","\r\n","# load all test reviews\r\n","negative_docs = process_docs('/Users/user/Desktop/txt_sentoken/neg', False)\r\n","positive_docs = process_docs('/Users/user/Desktop/txt_sentoken/pos', False)\r\n","testX = negative_docs + positive_docs\r\n","testY = [0 for _ in range(10)] + [1 for _ in range(10)]\r\n","save_dataset([testX,testY], 'test.pkl')\r\n","\r\n","# load a clean dataset\r\n","from pickle import load\r\n","def load_dataset(filename):\r\n","\treturn load(open(filename, 'rb'))\r\n"," \r\n","trainLines, trainLabels = load_dataset('/Users/user/Desktop/dccnn-master/train.pkl')\r\n","\r\n","from keras.preprocessing.text import Tokenizer\r\n","\r\n","\r\n","# fit a tokenizer\r\n","def create_tokenizer(lines):\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n","\r\n","# calculate the maximum document length\r\n","#def max_length(lines):\r\n","#\treturn max([len(s.split()) for s in lines])\r\n","\r\n","\r\n","# encode a list of lines\r\n","def encode_text(tokenizer, lines, length):\r\n","\t# integer encode\r\n","\tencoded = tokenizer.texts_to_sequences(lines)\r\n","\t# pad encoded sequences\r\n","\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n","\treturn padded\r\n","\r\n","\r\n","# calculate vocabulary size\r\n","#tokenizer = Tokenizer()\r\n","#vocab_size = len(tokenizer.word_index) + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Ai1nSxQuuhN"},"source":["from nltk.corpus import stopwords\r\n","import string\r\n"," \r\n","# load doc into memory\r\n","def load_doc(filename):\r\n","\t# open the file as read only\r\n","\tfile = open(filename, 'r')\r\n","\t# read all text\r\n","\ttext = file.read()\r\n","\t# close the file\r\n","\tfile.close()\r\n","\treturn text\r\n"," \r\n","# turn a doc into clean tokens\r\n","def clean_doc(doc):\r\n","\t# split into tokens by white space\r\n","\ttokens = doc.split()\r\n","\t# remove punctuation from each token\r\n","\ttable = str.maketrans('', '', string.punctuation)\r\n","\ttokens = [w.translate(table) for w in tokens]\r\n","\t# remove remaining tokens that are not alphabetic\r\n","\ttokens = [word for word in tokens if word.isalpha()]\r\n","\t# filter out stop words\r\n","\t#stop_words = set(stopwords.words('english'))\r\n","\t#tokens = [w for w in tokens if not w in stop_words]\r\n","\t# filter out short tokens\r\n","\ttokens = [word for word in tokens if len(word) > 1]\r\n","\treturn tokens\r\n"," \r\n","# load the document\r\n","filename = 'neg.txt'\r\n","text = load_doc(filename)\r\n","tokens = clean_doc(text)\r\n","print(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"Nx_m2Gp02S0n","executionInfo":{"status":"error","timestamp":1608072225229,"user_tz":-60,"elapsed":8154,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"0d2522ce-1aba-4284-f0f4-770b5bb299be"},"source":["import numpy as np\r\n","from pickle import load\r\n","from numpy import array\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.utils.vis_utils import plot_model\r\n","from keras.models import Model\r\n","from keras.layers import Input\r\n","from keras.layers import Dense\r\n","from keras.layers import Flatten\r\n","from keras.layers import Dropout\r\n","from keras.layers import Embedding\r\n","from keras.layers.convolutional import Conv1D\r\n","from keras.layers.convolutional import MaxPooling1D\r\n","from keras.layers.merge import concatenate\r\n","\r\n","# load a clean dataset\r\n","def load_dataset(filename):\r\n","\treturn load(open(filename, 'rb'))\r\n","\r\n","# fit a tokenizer\r\n","def create_tokenizer(lines):\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n","\r\n","# calculate the maximum document length\r\n","def max_length(lines):\r\n","  for s in lines:\r\n","    s=str(s)\r\n","    Type = s.split(\",\")\r\n","  return max([len(Type) for s in lines])\r\n","\t#return max([len(Type) for s in lines])\r\n","\r\n","# encode a list of lines\r\n","def encode_text(tokenizer, lines, length):\r\n","\t# integer encode\r\n","\tencoded = tokenizer.texts_to_sequences(lines)\r\n","\t# pad encoded sequences\r\n","\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n","\treturn padded\r\n","\r\n","# define the model\r\n","def define_model(length, vocab_size):\r\n","\t# channel 1\r\n","\tinputs1 = Input(shape=(length,))\r\n","\tembedding1 = Embedding(vocab_size, 40)(inputs1)\r\n","\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n","\tdrop1 = Dropout(0.5)(conv1)\r\n","\tpool1 = MaxPooling1D(pool_size=2)(drop1)\r\n","\tflat1 = Flatten()(pool1)\r\n","\t# channel 2\r\n","\tinputs2 = Input(shape=(length,))\r\n","\tembedding2 = Embedding(vocab_size, 40)(inputs2)\r\n","\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\r\n","\tdrop2 = Dropout(0.5)(conv2)\r\n","\tpool2 = MaxPooling1D(pool_size=2)(drop2)\r\n","\tflat2 = Flatten()(pool2)\r\n","\t# channel 3\r\n","\tinputs3 = Input(shape=(length,))\r\n","\tembedding3 = Embedding(vocab_size,40)(inputs3)\r\n","\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\r\n","\tdrop3 = Dropout(0.5)(conv3)\r\n","\tpool3 = MaxPooling1D(pool_size=2)(drop3)\r\n","\tflat3 = Flatten()(pool3)\r\n","\t# merge\r\n","\tmerged = concatenate([flat1, flat2, flat3])\r\n","\t# interpretation\r\n","\tdense1 = Dense(10, activation='relu')(merged)\r\n","\toutputs = Dense(1, activation='sigmoid')(dense1)\r\n","\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\r\n","\t# compile\r\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\t# summarize\r\n","\tprint(model.summary())\r\n","\tplot_model(model, show_shapes=True, to_file='multichannel.png')\r\n","\treturn model\r\n","#model.save('model.h5')\r\n","\r\n","# load training dataset\r\n","trainLines, trainLabels = load_dataset('train.pkl')\r\n","# create tokenizer\r\n","tokenizer = create_tokenizer(trainLines)\r\n","# calculate max document length\r\n","#length = 40\r\n","length = max_length(trainLines)\r\n","# calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print(length)\r\n","print('Max document length: %d' % length)\r\n","print('Vocabulary size: %d' % vocab_size)\r\n","# encode data\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","#testX = encode_text(tokenizer, testLines, length)\r\n","print(trainX.shape, testX.shape)\r\n"," \r\n","#trainX=np.reshape(trainX,(5, 11087))\r\n","trainLabels=np.reshape(trainLabels,(20, 90))\r\n","print(trainX.shape)\r\n","\r\n","# define model\r\n","model = define_model(length, vocab_size)\r\n","# fit model\r\n","model.fit([trainX,trainX], array(trainLabels),validation_data=(testX, testX,t), epochs=10, batch_size=16)\r\n","# save the model\r\n","model.save('model.h5')\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["55435\n","Max document length: 55435\n","Vocabulary size: 5411\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-12f764ff1013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m#testX = encode_text(tokenizer, testLines, length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m#trainX=np.reshape(trainX,(5, 11087))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'testX' is not defined"]}]},{"cell_type":"code","metadata":{"id":"0MSnL2_9YId4"},"source":["from keras.models import Sequential\r\n","from keras.layers import Convolution2D\r\n","from keras.layers import MaxPooling2D\r\n","from keras.layers import Flatten\r\n","from keras.layers import Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":493},"id":"WGmv9fIc07Nu","executionInfo":{"status":"error","timestamp":1607976163093,"user_tz":-60,"elapsed":3612,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"96a2ad9e-37f0-4610-c042-b0b95ad50917"},"source":["from pickle import load\r\n","from numpy import array\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.models import load_model\r\n"," \r\n","# load a clean dataset\r\n","def load_dataset(filename):\r\n","\treturn load(open(filename, 'rb'))\r\n"," \r\n","# fit a tokenizer\r\n","def create_tokenizer(lines):\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n"," \r\n","# calculate the maximum document length\r\n","def max_length(lines):\r\n","  for s in lines:\r\n","    s=str(s)\r\n","    Type = s.split(\",\")\r\n","  return max([len(Type) for s in lines])\r\n"," \r\n","# encode a list of lines\r\n","def encode_text(tokenizer, lines, length):\r\n","\t# integer encode\r\n","\tencoded = tokenizer.texts_to_sequences(lines)\r\n","\t# pad encoded sequences\r\n","\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n","\treturn padded\r\n"," \r\n","# load datasets\r\n","trainLines, trainLabels = load_dataset('train.pkl')\r\n","testLines, testLabels = load_dataset('test.pkl')\r\n"," \r\n","# create tokenizer\r\n","tokenizer = create_tokenizer(trainLines)\r\n","# calculate max document length\r\n","length = max_length(trainLines)\r\n","# calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print('Max document length: %d' % length)\r\n","print('Vocabulary size: %d' % vocab_size)\r\n","# encode data\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","testX = encode_text(tokenizer, testLines, length)\r\n","print(trainX.shape, testX.shape)\r\n"," \r\n","# load the model\r\n","model = load_model('model.h5')\r\n"," \r\n","# evaluate model on training dataset\r\n","loss, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\r\n","print('Train Accuracy: %f' % (acc*100))\r\n"," \r\n","# evaluate model on test dataset dataset\r\n","loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\r\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max document length: 55435\n","Vocabulary size: 5411\n","(20, 55435) (0, 55435)\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-392e628601a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# evaluate model on training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[1;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 20, 20, 20\n  y sizes: 1800\nPlease provide data which shares the same first dimension."]}]}]}