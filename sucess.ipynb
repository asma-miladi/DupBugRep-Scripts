{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"sucess.ipynb","provenance":[{"file_id":"15UuZD3faenMQyocBgJqKGJ-4fo3A8SXy","timestamp":1608164244360}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNIOL+6+fV9R0tOkWEJZiml"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GPDZhRXkut5S"},"source":["convert csv to txt"]},{"cell_type":"code","metadata":{"id":"b_wKOs2JpJV5"},"source":["with open('test.csv', 'r') as inp, open('test.txt', 'w') as out:\n","    for line in inp:\n","        line = line.replace(',', ':')\n","        out.write(line)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMrCckxWRn59"},"source":["!pip install q keras==2.3.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtzT5J5J_a5d"},"source":["!pip install tensorflow==2.1.0\r\n","import tensorflow as tf\r\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXy_G6rkfcYL"},"source":["from tensorflow.keras import optimizers\n","#from tensorflow.python.keras.optimizers import TFOptimizer\n","import numpy as np\n","from nltk.corpus import stopwords\n","import string\n","from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","#from tensorflow.keras.layers.convolutional import Conv2D\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Activation\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.merge import concatenate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXQVJhJ9yLeJ"},"source":["import numpy as np\r\n","from pickle import load\r\n","from numpy import array\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.utils.vis_utils import plot_model\r\n","from keras.models import Model\r\n","from keras.layers import Input\r\n","from keras.layers import Dense\r\n","from keras.layers import Flatten\r\n","from keras.layers import Dropout\r\n","from keras.layers import Embedding\r\n","from keras.layers.convolutional import Conv1D\r\n","from keras.layers.convolutional import MaxPooling1D\r\n","from keras.layers.merge import concatenate\r\n","\r\n","# load a clean dataset\r\n","def load_dataset(filename):\r\n","\treturn load(open(filename, 'rb'))\r\n","\r\n","# fit a tokenizer\r\n","def create_tokenizer(lines):\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n","\r\n","# calculate the maximum document length\r\n","def max_length(lines):\r\n","  for s in lines:\r\n","    s=str(s)\r\n","    Type = s.split(\",\")\r\n","  return max([len(Type) for s in lines])\r\n","\t#return max([len(Type) for s in lines])\r\n","\r\n","# encode a list of lines\r\n","def encode_text(tokenizer, lines, length):\r\n","\t# integer encode\r\n","\tencoded = tokenizer.texts_to_sequences(lines)\r\n","\t# pad encoded sequences\r\n","\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n","\treturn padded\r\n","\r\n","# define the model\r\n","def define_model(length, vocab_size):\r\n","\t# channel 1\r\n","\tinputs1 = Input(shape=(length,))\r\n","\tembedding1 = Embedding(vocab_size, 100)(inputs1)\r\n","\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n","\tdrop1 = Dropout(0.5)(conv1)\r\n","\tpool1 = MaxPooling1D(pool_size=2)(drop1)\r\n","\tflat1 = Flatten()(pool1)\r\n","\t# channel 2\r\n","\tinputs2 = Input(shape=(length,))\r\n","\tembedding2 = Embedding(vocab_size, 100)(inputs2)\r\n","\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\r\n","\tdrop2 = Dropout(0.5)(conv2)\r\n","\tpool2 = MaxPooling1D(pool_size=2)(drop2)\r\n","\tflat2 = Flatten()(pool2)\r\n","\t# channel 3\r\n","\tinputs3 = Input(shape=(length,))\r\n","\tembedding3 = Embedding(vocab_size,100)(inputs3)\r\n","\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\r\n","\tdrop3 = Dropout(0.5)(conv3)\r\n","\tpool3 = MaxPooling1D(pool_size=2)(drop3)\r\n","\tflat3 = Flatten()(pool3)\r\n","\t# merge\r\n","\tmerged = concatenate([flat1, flat2, flat3])\r\n","\t# interpretation\r\n","\tdense1 = Dense(10, activation='relu')(merged)\r\n","\toutputs = Dense(1, activation='sigmoid')(dense1)\r\n","\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\r\n","\t# compile\r\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\t# summarize\r\n","\tprint(model.summary())\r\n","\tplot_model(model, show_shapes=True, to_file='multichannel.png')\r\n","\treturn model\r\n","\r\n","# load training dataset\r\n","trainLines, trainLabels = load_dataset('train.pkl')\r\n","testLines, testLabels = load_dataset('test.pkl')\r\n","# create tokenizer\r\n","tokenizer = create_tokenizer(trainLines)\r\n","# calculate max document length\r\n","#length = 40\r\n","length = max_length(trainLines)\r\n","# calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","#print(length)\r\n","#print('Max document length: %d' % length)\r\n","#print('Vocabulary size: %d' % vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gIyTQyg9yhmh"},"source":["Please provide data which shares the same first dimension."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxonVcBZyWAa","executionInfo":{"status":"ok","timestamp":1608147828785,"user_tz":-60,"elapsed":4414,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"5ad63544-700e-430e-fc7d-9517f6157aaf"},"source":["\r\n","# encode data,\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","\r\n","print(trainX.shape)\r\n","#print(trainLabels.shape)\r\n","#trainX=np.reshape(trainX,(1, 5543500))\r\n","#trainLabels=np.reshape(trainLabels,(1, 40))\r\n","print(trainX.shape)\r\n","\r\n","# define model\r\n","model = define_model(length, vocab_size)\r\n","# fit model\r\n","#model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\r\n","# save the model\r\n","model.save('model.h5')\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(100, 55435)\n","(100, 55435)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 55435, 100)   541100      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 55435, 100)   541100      input_2[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, 55435, 100)   541100      input_3[0][0]                    \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 55432, 32)    12832       embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","conv1d_2 (Conv1D)               (None, 55430, 32)    19232       embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","conv1d_3 (Conv1D)               (None, 55428, 32)    25632       embedding_3[0][0]                \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 55432, 32)    0           conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 55430, 32)    0           conv1d_2[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 55428, 32)    0           conv1d_3[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling1d_1 (MaxPooling1D)  (None, 27716, 32)    0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling1d_2 (MaxPooling1D)  (None, 27715, 32)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling1d_3 (MaxPooling1D)  (None, 27714, 32)    0           dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 886912)       0           max_pooling1d_1[0][0]            \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 886880)       0           max_pooling1d_2[0][0]            \n","__________________________________________________________________________________________________\n","flatten_3 (Flatten)             (None, 886848)       0           max_pooling1d_3[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 2660640)      0           flatten_1[0][0]                  \n","                                                                 flatten_2[0][0]                  \n","                                                                 flatten_3[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 10)           26606410    concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n","==================================================================================================\n","Total params: 28,287,417\n","Trainable params: 28,287,417\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvIEHaDXikq1","executionInfo":{"status":"ok","timestamp":1608138867765,"user_tz":-60,"elapsed":3318,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"eb87d12e-a147-4963-c88a-fb7c3411831e"},"source":["trainX = encode_text(tokenizer, trainLines, length)\r\n","testX = encode_text(tokenizer, testLines, length)\r\n","print(trainX.shape, testX.shape)\r\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","text":["(100, 55435) (1, 55435)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sry9nr30jgKs","executionInfo":{"status":"ok","timestamp":1608139026457,"user_tz":-60,"elapsed":7448,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"eb3eaf0f-0349-419d-e03c-8f9eb4305120"},"source":["trainLines, trainLabels = load_dataset('train.pkl')\r\n","testLines, testLabels = load_dataset('test.pkl')\r\n"," \r\n","# create tokenizer\r\n","tokenizer = create_tokenizer(trainLines)\r\n","# calculate max document length\r\n","length = max_length(trainLines)\r\n","# calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print('Max document length: %d' % length)\r\n","print('Vocabulary size: %d' % vocab_size)\r\n","# encode data\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","testX = encode_text(tokenizer, testLines, length)\r\n","print(trainX.shape, testX.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max document length: 55435\n","Vocabulary size: 5411\n","(100, 55435) (1, 55435)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vNVp_U7Gjh_X","executionInfo":{"status":"error","timestamp":1608139155818,"user_tz":-60,"elapsed":7667,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"12dc0cfa-bea4-4ef6-eaf3-8a63ebfe3120"},"source":["from pickle import load\r\n","from numpy import array\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.models import load_model\r\n"," \r\n","# load a clean dataset\r\n","def load_dataset(filename):\r\n","\treturn load(open(filename, 'rb'))\r\n"," \r\n","# fit a tokenizer\r\n","def create_tokenizer(lines):\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n"," \r\n","# calculate the maximum document length\r\n","def max_length(lines):\r\n","  for s in lines:\r\n","    s=str(s)\r\n","    Type = s.split(\",\")\r\n","  return max([len(Type) for s in lines])\r\n"," \r\n","# encode a list of lines\r\n","def encode_text(tokenizer, lines, length):\r\n","\t# integer encode\r\n","\tencoded = tokenizer.texts_to_sequences(lines)\r\n","\t# pad encoded sequences\r\n","\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n","\treturn padded\r\n"," \r\n","# load datasets\r\n","trainLines, trainLabels = load_dataset('train.pkl')\r\n","testLines, testLabels = load_dataset('test.pkl')\r\n"," \r\n","# create tokenizer\r\n","tokenizer = create_tokenizer(trainLines)\r\n","# calculate max document length\r\n","length = max_length(trainLines)\r\n","# calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print('Max document length: %d' % length)\r\n","print('Vocabulary size: %d' % vocab_size)\r\n","# encode data\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","testX = encode_text(tokenizer, testLines, length)\r\n","print(trainX.shape, testX.shape)\r\n"," \r\n","# load the model\r\n","model = define_model(length, vocab_size) \r\n","# evaluate model on training dataset\r\n","loss, acc = model.evaluate([trainX,trainX,trainX], array(testX), verbose=0)\r\n","print('Train Accuracy: %f' % (acc*100))\r\n"," \r\n","# evaluate model on test dataset dataset\r\n","loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\r\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max document length: 55435\n","Vocabulary size: 5411\n","(100, 55435) (1, 55435)\n","Model: \"model_15\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_43 (InputLayer)           (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","input_44 (InputLayer)           (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","input_45 (InputLayer)           (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","embedding_43 (Embedding)        (None, 55435, 100)   541100      input_43[0][0]                   \n","__________________________________________________________________________________________________\n","embedding_44 (Embedding)        (None, 55435, 100)   541100      input_44[0][0]                   \n","__________________________________________________________________________________________________\n","embedding_45 (Embedding)        (None, 55435, 100)   541100      input_45[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d_43 (Conv1D)              (None, 55432, 32)    12832       embedding_43[0][0]               \n","__________________________________________________________________________________________________\n","conv1d_44 (Conv1D)              (None, 55430, 32)    19232       embedding_44[0][0]               \n","__________________________________________________________________________________________________\n","conv1d_45 (Conv1D)              (None, 55428, 32)    25632       embedding_45[0][0]               \n","__________________________________________________________________________________________________\n","dropout_43 (Dropout)            (None, 55432, 32)    0           conv1d_43[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_44 (Dropout)            (None, 55430, 32)    0           conv1d_44[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_45 (Dropout)            (None, 55428, 32)    0           conv1d_45[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling1d_43 (MaxPooling1D) (None, 27716, 32)    0           dropout_43[0][0]                 \n","__________________________________________________________________________________________________\n","max_pooling1d_44 (MaxPooling1D) (None, 27715, 32)    0           dropout_44[0][0]                 \n","__________________________________________________________________________________________________\n","max_pooling1d_45 (MaxPooling1D) (None, 27714, 32)    0           dropout_45[0][0]                 \n","__________________________________________________________________________________________________\n","flatten_43 (Flatten)            (None, 886912)       0           max_pooling1d_43[0][0]           \n","__________________________________________________________________________________________________\n","flatten_44 (Flatten)            (None, 886880)       0           max_pooling1d_44[0][0]           \n","__________________________________________________________________________________________________\n","flatten_45 (Flatten)            (None, 886848)       0           max_pooling1d_45[0][0]           \n","__________________________________________________________________________________________________\n","concatenate_15 (Concatenate)    (None, 2660640)      0           flatten_43[0][0]                 \n","                                                                 flatten_44[0][0]                 \n","                                                                 flatten_45[0][0]                 \n","__________________________________________________________________________________________________\n","dense_29 (Dense)                (None, 10)           26606410    concatenate_15[0][0]             \n","__________________________________________________________________________________________________\n","dense_30 (Dense)                (None, 1)            11          dense_29[0][0]                   \n","==================================================================================================\n","Total params: 28,287,417\n","Trainable params: 28,287,417\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-042d81e5c30d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# evaluate model on training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_30 to have shape (1,) but got array with shape (55435,)"]}]},{"cell_type":"code","metadata":{"id":"Kb84ThSojB1w"},"source":["# create tokenizer\r\n","tokenizer = create_tokenizer(trainLines)\r\n","# calculate max document length\r\n","length = max_length(trainLines)\r\n","# calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print('Max document length: %d' % length)\r\n","print('Vocabulary size: %d' % vocab_size)\r\n","# encode data\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","testX = encode_text(tokenizer, testLines, length)\r\n","print(trainX.shape, testX.shape)\r\n"," \r\n","# load the model\r\n","model = define_model(length, vocab_size)\r\n"," \r\n","# evaluate model on training dataset\r\n","loss, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\r\n","print('Train Accuracy: %f' % (acc*100))\r\n"," \r\n","# evaluate model on test dataset dataset\r\n","loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\r\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"_8wbdmZZylZk","executionInfo":{"status":"error","timestamp":1608148425907,"user_tz":-60,"elapsed":3329,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"b5eade38-1d8b-435f-e425-ab8f5eb11522"},"source":["# encode data,\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","trainX=np.reshape(trainX,(100, 55435))\r\n","trainLabels=np.reshape(trainLabels,(40, 1))\r\n","print(trainX.shape)\r\n","\r\n","# define model\r\n","model = define_model(length, vocab_size)\r\n","# fit model\r\n","model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\r\n","# save the model\r\n","model.save('model.h5')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-974676593996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m55435\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 100 into shape (40,1)"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lDkmjyEjymz4","executionInfo":{"status":"ok","timestamp":1608136955766,"user_tz":-60,"elapsed":3644,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"92a419ed-5f4c-429c-810a-6488ff7eabb4"},"source":["# encode data\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","trainX=np.reshape(trainX,(100, 55435))\r\n","trainLabels=np.reshape(trainLabels,(40, 1))\r\n","print(trainX.shape)\r\n","print(trainLabels.shape)\r\n","\r\n","# define model\r\n","#model = define_model(length, vocab_size)\r\n","# fit model\r\n","#model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\r\n","# save the model\r\n","model.save('model.h5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(100, 55435)\n","(40, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rSuVn66Bl6Mg"},"source":["*******************************************************************************************************************************************************************************************************************************************************"]},{"cell_type":"code","metadata":{"id":"U699fpQLFUYe"},"source":["*********************************************************************************************************"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RG5zB5XDmNwK"},"source":["import numpy as np\r\n","from pickle import load\r\n","from numpy import array\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.utils.vis_utils import plot_model\r\n","from keras.models import Model\r\n","from keras.layers import Input\r\n","from keras.layers import Dense\r\n","from keras.layers import Flatten\r\n","from keras.layers import Dropout\r\n","from keras.layers import Embedding\r\n","from keras.layers.convolutional import Conv1D\r\n","from keras.layers.convolutional import MaxPooling1D\r\n","from keras.layers.merge import concatenate\r\n","\r\n","def load_dataset(filename):\r\n","\treturn load(open(filename, 'rb'))\r\n"," \r\n","trainLines, trainLabels = load_dataset('train.pkl')\r\n","testLines, testLabels = load_dataset('test.pkl')\r\n","tokenizer = create_tokenizer(trainLines)\r\n","\r\n","\r\n","# fit a tokenizer\r\n","def create_tokenizer(lines):\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n","# calculate the maximum document length\r\n","def max_length(lines):\r\n","  for s in lines:\r\n","    s=str(s)\r\n","    Type = s.split(\",\")\r\n","  return max([len(Type) for s in lines])\r\n","\r\n","  # calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","\r\n","# encode a list of lines\r\n","def encode_text(tokenizer, lines, length):\r\n","\t# integer encode\r\n","\tencoded = tokenizer.texts_to_sequences(lines)\r\n","\t# pad encoded sequences\r\n","\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n","\treturn padded"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"brGAvf8mnTHh"},"source":["# define the model\r\n","def define_model(length, vocab_size):\r\n","\t# channel 1\r\n","\tinputs1 = Input(shape=(length,))\r\n","\tembedding1 = Embedding(vocab_size, 100)(inputs1)\r\n","\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n","\tdrop1 = Dropout(0.5)(conv1)\r\n","\tpool1 = MaxPooling1D(pool_size=2)(drop1)\r\n","\tflat1 = Flatten()(pool1)\r\n","\t# channel 2\r\n","\tinputs2 = Input(shape=(length,))\r\n","\tembedding2 = Embedding(vocab_size, 100)(inputs2)\r\n","\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\r\n","\tdrop2 = Dropout(0.5)(conv2)\r\n","\tpool2 = MaxPooling1D(pool_size=2)(drop2)\r\n","\tflat2 = Flatten()(pool2)\r\n","\t# channel 3\r\n","\tinputs3 = Input(shape=(length,))\r\n","\tembedding3 = Embedding(vocab_size, 100)(inputs3)\r\n","\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\r\n","\tdrop3 = Dropout(0.5)(conv3)\r\n","\tpool3 = MaxPooling1D(pool_size=2)(drop3)\r\n","\tflat3 = Flatten()(pool3)\r\n","\t# merge\r\n","\tmerged = concatenate([flat1, flat2, flat3])\r\n","\t# interpretation\r\n","\tdense1 = Dense(10, activation='relu')(merged)\r\n","\toutputs = Dense(1, activation='sigmoid')(dense1)\r\n","\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\r\n","\t# compile\r\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\t# summarize\r\n","\tprint(model.summary())\r\n","\tplot_model(model, show_shapes=True, to_file='multichannel.png')\r\n","\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pv7Nx9Glndyt","executionInfo":{"status":"ok","timestamp":1608156223453,"user_tz":-60,"elapsed":7241,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"70792aad-7504-4121-b477-341581151cda"},"source":["# load datasets\r\n","trainLines, trainLabels = load_dataset('train.pkl')\r\n","testLines, testLabels = load_dataset('test.pkl')\r\n","\r\n","# create tokenizer\r\n","tokenizer = create_tokenizer(trainLines)\r\n","# calculate max document length\r\n","length = max_length(trainLines)\r\n","# calculate vocabulary size\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print('Max document length: %d' % length)\r\n","print('Vocabulary size: %d' % vocab_size)\r\n","# encode data\r\n","trainX = encode_text(tokenizer, trainLines, length)\r\n","testX = encode_text(tokenizer, testLines, length)\r\n","print(trainX.ndim, testX.ndim)\r\n","print(trainLines.count)\r\n","print(trainLabels.count)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max document length: 55435\n","Vocabulary size: 5411\n","2 2\n","<built-in method count of list object at 0x7f9489b1fbc8>\n","<built-in method count of list object at 0x7f94862f8188>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQwa4FSJA1Hw","executionInfo":{"status":"ok","timestamp":1608159601511,"user_tz":-60,"elapsed":1091,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"086a8c62-e59d-4ea4-a7a2-8517bf2a76db"},"source":["trainLabels = np.array([0] * 50 + [1] * 50)\r\n","trainLabels\r\n","#trainLines"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v3JqdWDtyQ-R","executionInfo":{"status":"ok","timestamp":1608159774426,"user_tz":-60,"elapsed":101073,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"19422a6f-ca11-4ebd-88e5-b4d225e7098b"},"source":["model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","100/100 [==============================] - 10s 105ms/step - loss: 25.3960 - accuracy: 0.5200\n","Epoch 2/10\n","100/100 [==============================] - 11s 107ms/step - loss: 0.6931 - accuracy: 0.5000\n","Epoch 3/10\n","100/100 [==============================] - 11s 115ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 4/10\n","100/100 [==============================] - 10s 98ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 5/10\n","100/100 [==============================] - 9s 94ms/step - loss: 0.6931 - accuracy: 0.5000\n","Epoch 6/10\n","100/100 [==============================] - 9s 95ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 7/10\n","100/100 [==============================] - 9s 95ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 8/10\n","100/100 [==============================] - 9s 95ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 9/10\n","100/100 [==============================] - 10s 96ms/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 10/10\n","100/100 [==============================] - 10s 95ms/step - loss: 0.6932 - accuracy: 0.5000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7f94a3b8f828>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GXsOOm1toheH","executionInfo":{"status":"error","timestamp":1608159612181,"user_tz":-60,"elapsed":7313,"user":{"displayName":"SSBSE Admin","photoUrl":"","userId":"11167642343971864398"}},"outputId":"f5e4fbb6-3fe4-4063-ec6b-8acd10d28c2d"},"source":["model=define_model(length,vocab_size)\r\n","# evaluate model on training dataset\r\n","loss, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\r\n","#print('Train Accuracy: %f' % (acc*100))\r\n"," \r\n","# evaluate model on test dataset dataset\r\n","loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\r\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","input_8 (InputLayer)            (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","input_9 (InputLayer)            (None, 55435)        0                                            \n","__________________________________________________________________________________________________\n","embedding_7 (Embedding)         (None, 55435, 100)   541100      input_7[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_8 (Embedding)         (None, 55435, 100)   541100      input_8[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_9 (Embedding)         (None, 55435, 100)   541100      input_9[0][0]                    \n","__________________________________________________________________________________________________\n","conv1d_7 (Conv1D)               (None, 55432, 32)    12832       embedding_7[0][0]                \n","__________________________________________________________________________________________________\n","conv1d_8 (Conv1D)               (None, 55430, 32)    19232       embedding_8[0][0]                \n","__________________________________________________________________________________________________\n","conv1d_9 (Conv1D)               (None, 55428, 32)    25632       embedding_9[0][0]                \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 55432, 32)    0           conv1d_7[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 55430, 32)    0           conv1d_8[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_9 (Dropout)             (None, 55428, 32)    0           conv1d_9[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling1d_7 (MaxPooling1D)  (None, 27716, 32)    0           dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling1d_8 (MaxPooling1D)  (None, 27715, 32)    0           dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling1d_9 (MaxPooling1D)  (None, 27714, 32)    0           dropout_9[0][0]                  \n","__________________________________________________________________________________________________\n","flatten_7 (Flatten)             (None, 886912)       0           max_pooling1d_7[0][0]            \n","__________________________________________________________________________________________________\n","flatten_8 (Flatten)             (None, 886880)       0           max_pooling1d_8[0][0]            \n","__________________________________________________________________________________________________\n","flatten_9 (Flatten)             (None, 886848)       0           max_pooling1d_9[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 2660640)      0           flatten_7[0][0]                  \n","                                                                 flatten_8[0][0]                  \n","                                                                 flatten_9[0][0]                  \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 10)           26606410    concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 1)            11          dense_5[0][0]                    \n","==================================================================================================\n","Total params: 28,287,417\n","Trainable params: 28,287,417\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-97b7ee2302e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# evaluate model on test dataset dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1350\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    242\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         raise ValueError('All sample_weight arrays should have '\n","\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 1 input samples and 40 target samples."]}]}]}